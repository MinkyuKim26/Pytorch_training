{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 작물 잎 사진으로 질병 분류하기\n",
    "\n",
    "## 작물 잎 사진의 종류와 질병 유무를 분류하는 모델 설계, 학습"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. 데이터 분할\n",
    "\n",
    "### 데이터셋을 학습용, 검증용, 테스트용으로 분리"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import os\n",
    "import shutil # 파일과 파일 모음에 대한 여러 가지 고수준 연산을 제공(파일 복사, 삭제 등)\n",
    "\n",
    "# 원래 데이터셋이 있는 경로\n",
    "original_dataset_dir = '/Users/minguinho/Documents/AI_Datasets/crop_leaf_original_dataset'\n",
    "classes_list = os.listdir(original_dataset_dir) # 원래 데이터셋에 클래스 별로 데이터가 저장되어 있다. 각 클래스별 폴더 경로를 classes_list에 저장한다 \n",
    "\n",
    "# 분류할 데이터를 저장할 곳\n",
    "base_dir = '/Users/minguinho/Documents/AI_Datasets/crops_leaf_dataset'\n",
    "os.mkdir(base_dir)\n",
    "\n",
    "# 학습, 검증, 테스트용 데이터 저장할 곳 생성\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "os.mkdir(train_dir)\n",
    "\n",
    "validation_dir = os.path.join(base_dir, 'val')\n",
    "os.mkdir(validation_dir)\n",
    "\n",
    "test_dir = os.path.join(base_dir, 'test')\n",
    "os.mkdir(test_dir)\n",
    "\n",
    "\n",
    "# 클래스별로 사진을 저장하기 위해 폴더 생성\n",
    "for clss in classes_list :\n",
    "    os.mkdir(os.path.join(train_dir, clss))\n",
    "    os.mkdir(os.path.join(validation_dir, clss))\n",
    "    os.mkdir(os.path.join(test_dir, clss))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import math\n",
    "\n",
    "# 클래스별로 저장된 사진들을 학습, 검증, 테스트용 데이터셋에 저장한다\n",
    "for clss in classes_list:\n",
    "    path = os.path.join(original_dataset_dir, clss) # 원래 데이터셋/클래스이름\n",
    "    fnames = os.listdir(path) # 클래스별로 저장되어있는 파일의 경로들을 여기에 저장한다. 예) 건강한 토마토 잎들의 경로들을 fnames에 저장 \n",
    "\n",
    "    # math.floor() : 지정된 숫자보다 작거나 같은 가장 큰 정수를 반환\n",
    "    train_size = math.floor(len(fnames) * 0.6)\n",
    "    validation_size = math.floor(len(fnames) * 0.2)\n",
    "    test_size = math.floor(len(fnames) * 0.2)\n",
    "\n",
    "    # 파일을 학습, 검증, 테스트용으로 나눠서 저장\n",
    "    train_fnames = fnames[:train_size]\n",
    "    print('Train size(', clss, ') : ', len(train_fnames))\n",
    "    for fname in train_fnames:\n",
    "        src = os.path.join(path, fname)\n",
    "        dst = os.path.join(os.path.join(train_dir, clss), fname) # 데이터셋/클래스이름/파일명\n",
    "        shutil.copyfile(src, dst) # 복사해서 저장\n",
    "\n",
    "    validation_fnames = fnames[train_size:(train_size + validation_size)]\n",
    "    print('Validation size(', clss, ') : ', len(validation_fnames))\n",
    "    for fname in validation_fnames:\n",
    "        src = os.path.join(path, fname)\n",
    "        dst = os.path.join(os.path.join(validation_dir, clss), fname)\n",
    "        shutil.copyfile(src, dst) # 복사해서 저장\n",
    "\n",
    "    test_fnames = fnames[validation_size:(train_size + validation_size + test_size)]\n",
    "    print('Test size(', clss, ') : ', len(test_fnames))\n",
    "    for fname in test_fnames:\n",
    "        src = os.path.join(path, fname)\n",
    "        dst = os.path.join(os.path.join(test_dir, clss), fname)\n",
    "        shutil.copyfile(src, dst) # 복사해서 저장"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 데이터 전처리"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import torch\n",
    "\n",
    "USE_CUDA = torch.coda.is_avaliable() # CUDA를 사용할 수 있는가?\n",
    "DEVICE = torch.device('cuda' if USE_CUDA else 'cpu') # CUDA를 사용할 수 있으면 CUDA를 사용하고 아니면 cpu 사용\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "EPOCH = 30\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "transform_base = transforms.Compose([transforms.Resize((64, 64)), transforms.ToTensor()])\n",
    "\n",
    "train_dataset = ImageFolder(root = '/Users/minguinho/Documents/AI_Datasets/crops_leaf_dataset/train', transform = transform_base)\n",
    "val_dataset   = ImageFolder(root = '/Users/minguinho/Documents/AI_Datasets/crops_leaf_dataset/val', transform = transform_base)\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 데이터셋을 미니배치 단위로 분리. shuffle은 미니배치 단위로 나눈걸 섞는지 물어보는거고 num_workers는 데이터를 불러올 때 사용할 subprocesses의 개수. 데이터를 불러와서 연산을 하는데 이 때 데이터 로딩에 얼마나 많은 GPU 자원을 사용할거냐 물어보는거다. 잘 정해야하는 hyper parameter다.\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size = BATCH_SIZE, shuffle = True, num_workers = 4)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size = BATCH_SIZE, shuffle=True, num_workers = 4)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 모델 설계"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.conv3 = nn.conv2d(64, 64, 3, padding=1)\n",
    "\n",
    "        self.fc1 = nn.Linear(4096, 512)\n",
    "        self.fc2 = nn.Linear(512, 33)\n",
    "\n",
    "    def forward(self, x) :\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = F.dropout(x, p=0.25, trainig=self.training) # self.training : 학습 모드일 때만 dropout을 사용\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = F.dropout(x, p=0.25, training=self.training)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = F.dropout(x, p=0.25, training=self.training)\n",
    "\n",
    "        x = x.view(-1, 4096) # Flatten\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "model_base = Net().to(DEVICE) # DEVICE에서 돌릴 model_base 생성\n",
    "optimizer = optim.Adam(model_base.parameters(), lr = 0.001) # 학습률 0.001로 model_base의 parameter들을 업데이트하는 Adam 생성"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 학습 함수, 평가 함수"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def train(model, train_loader, optimizer) : # 한 에포크만큼 훈련 실행\n",
    "    model.train() # 학습 모드로 변경\n",
    "    for batch_idx, (data, target) in enumerate(train_loader) : # 배치 단위로 parameter 업데이트\n",
    "        data, target = data.to(DEVICE), target.to(DEVICE) # to() : DEVICE에서 데이터 처리\n",
    "        optimizer.zero_grad() # torch 모듈에 있는 그레디언트 초기화\n",
    "        output = model(data)\n",
    "        loss = F.cross_entropy(output, target)\n",
    "        loss.backward() # loss에서 각 parameter에 해당하는 그레디언트 구함\n",
    "        optimizer.step() # 가중치 업데이트"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def evaluate(model, test_loader) : \n",
    "    model.eval() # 평가 모드(검증, 테스트 용도)로 변경\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad(): # 평가를 하며 모델의 parameter를 업데이트 하는걸 중단. with문에 속한 영역에서만 torch.no_grad()가 적용됨\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "            output = model(data)\n",
    "\n",
    "            test_loss +=F.cross_entropy(output, target, reduction = 'sum').item() # 계산값 양식이 Tensor다. item()은 Tensor에 담긴 값을 사용하겠다는 것. 예 : tensor([[ 1]]).item = 1\n",
    "            # reduction : Specifies the reduction to apply to the output. 'sum': the output will be summed. loss를 다 더한값을 반환 클래스별 loss가 있기 때문에 이걸 평균값(mean)을 낼지 다른걸 할지 결정. 기본값은 평균\n",
    "\n",
    "            pred = output.max(1, keepdim = True)[1] # 확률 제일 높은걸 사용. Returns the maximum value of all elements in Tensor이고 keepdim = True이면 output과 같은 양식으로 반환됨. 1은 dim=1인데 이는 the dimension to reduce = 1이라는 뜻과 같다.\n",
    "            # 그럼 [1]은 뭐지? \n",
    "\n",
    "            correct += preq.eq(target.view_as(pred)).sum().item() # target.view_as(pred) : pred와 같은 모양으로 정렬, eq는 두 텐서가 같은지 판단하는 비교 연산자로 같은면 1, 다르면 0 반환. 클래스별 확률값을 다 대조하는듯 한데 어짜피 한가지 객체에 대한 분류 모델이라 0 아니면 1이 된다. \n",
    "\n",
    "    test_loss /= len(test_loader.dataset) # 미니배치 단위로 구한걸 다 합쳤으니 이걸 미니배치 개수로 나눠 평균 loss를 구해줌. \n",
    "    test_accuracy = 100. * correct / len(test_loader.dataset) # 0~100% 중 하나가 나오겠지? \n",
    "\n",
    "    return test_loss, test_accuracy"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 학습 & 성능평가"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import copy\n",
    "import time \n",
    "\n",
    "def train_baseline(model, train_loader, val_loader, optimizer, num_epochs = 30) :\n",
    "    best_acc = 0.0\n",
    "    best_model_wts = copy.deepcopy(model.state_dict()) # 가장 성능이 좋은 모델을 저장하기 위함\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        since = time.time()\n",
    "        train(model, train_loader, optimizer)\n",
    "        # loss 휙득\n",
    "        train_loss, train_acc = evaluate(model, train_loader)\n",
    "        val_loss, val_acc = evaluate(model, val_loader)\n",
    "\n",
    "        if val_acc > best_acc : # 학습 과정에서 가장 적은 val_loss가 나온 모델을 최종 모델로 저장\n",
    "            best_acc = val_acc\n",
    "            best_model_wts = copy.deepcopy(model.state_dict()) # 저장\n",
    "\n",
    "        time_elapsed = time.time() - since # 학습에 소요된 시간\n",
    "        print('---------------- epoch {} ----------------'.format(epoch))\n",
    "\n",
    "        print('train Loss : {:.4f}, Accuracy: {:.2f}%'.format(train_loss, train_acc))\n",
    "        print('val Loss : {:.4f}, Accuracy: {:.2f}%'.format(val_loss, val_acc))\n",
    "        print('Completed in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model \n",
    "\n",
    "base = train_baseline(model_base, train_loader, val_loader, optimizer, EPOCH)\n",
    "\n",
    "torch.save(base, 'baseline.pt')"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}